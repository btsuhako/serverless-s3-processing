# Welcome to Serverless!
#
# This file is the main config file for your service.
# It's very minimal at this point and uses default values.
# You can always add more config options for more control.
# We've included some commented out config examples here.
# Just uncomment any of them to get that config option.
#
# For full config options, check the docs:
#    docs.serverless.com
#
# Happy Coding!

service: sls-s3-processing # NOTE: update this with your service name
# NOTE: the service name needs to be DNS compliant and all lowercase,
# since the S3 buckets created are derived from this name

# You can pin your service to only deploy with a specific Serverless version
# Check out our docs for more details
# frameworkVersion: "=X.X.X"

provider:
  name: aws
  runtime: python3.6
  timeout: 300 # optional, in seconds, default is 6

# you can overwrite defaults here
  stage: dev
  region: us-west-2

# you can add statements to the Lambda function's IAM Role here
  iamRoleStatements:
    - Effect: "Allow"
      Action:
        - "s3:*"
      Resource:
        # NOTE you can't refer to the LogicalID of S3BucketIncoming, otherwise
        # there will be a circular reference in CloudFormation
        Fn::Join: [ ":", [ arn, aws, s3, "", "", "${self:provider.environment.BUCKET_INCOMING}/*" ]]
    - Effect: "Allow"
      Action:
        - "s3:*"
      Resource:
        Fn::Join:
          - ""
          - - "arn:aws:s3:::"
            - "Ref" : "S3BucketBadData"
            - "/*"
    - Effect: "Allow"
      Action:
        - "s3:Put*"
      Resource:
        Fn::Join:
          - ""
          - - "arn:aws:s3:::"
            - "Ref" : "S3BucketProcessed"
            - "/*"
    - Effect: Allow
      Action:
        - dynamodb:DescribeTable
        - dynamodb:Query
        - dynamodb:Scan
        - dynamodb:GetItem
        - dynamodb:PutItem
        - dynamodb:UpdateItem
        - dynamodb:DeleteItem
      Resource:
        Fn::GetAtt: [ FileDynamoDbTable, Arn ]
  environment:
    TABLE_NAME: ${opt:stage, self:provider.stage}-${self:service}
    BUCKET_INCOMING: ${opt:stage, self:provider.stage}-${self:service}-s3incomingbucket
    BUCKET_BADDATA: ${opt:stage, self:provider.stage}-${self:service}-s3baddatasucket
    BUCKET_PROCESSED: ${opt:stage, self:provider.stage}-${self:service}-s3processedbucket
# you can define service wide environment variables here
#  environment:
#    variable1: value1

# you can add packaging information here
#package:
#  include:
#    - include-me.py
#    - include-me-dir/**
#  exclude:
#    - exclude-me.py
#    - exclude-me-dir/**

functions:
  gather:
    handler: handler.gather

#    The following are a few example events you can configure
#    NOTE: Please make sure to change your handler code to work with those events
#    Check the event documentation for details
    events:
#      - http:
#          path: users/create
#          method: get
#      - s3: ${env:BUCKET}
      - schedule: rate(10 minutes)
#      - sns: greeter-topic
#      - stream: arn:aws:dynamodb:region:XXXXXX:table/foo/stream/1970-01-01T00:00:00.000
#      - alexaSkill
#      - alexaSmartHome: amzn1.ask.skill.xx-xx-xx-xx
#      - iot:
#          sql: "SELECT * FROM 'some_topic'"
#      - cloudwatchEvent:
#          event:
#            source:
#              - "aws.ec2"
#            detail-type:
#              - "EC2 Instance State-change Notification"
#            detail:
#              state:
#                - pending
#      - cloudwatchLog: '/aws/lambda/hello'
#      - cognitoUserPool:
#          pool: MyUserPool
#          trigger: PreSignUp

# https://serverless.com/framework/docs/providers/aws/events/s3/
  process:
    handler: handler.process
    # NOTE this event isn't working yet, and causes a CloudFormation circular reference
    events:
      - s3:
          bucket: incoming
          event: s3:ObjectCreated:*
#    Define function environment variables here
#    environment:
#      variable2: value2

# you can add CloudFormation resource templates here
resources:
  Resources:
    # TODO default encryption? not possible with CloudFormation yet
    S3BucketIncoming:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: ${self:provider.environment.BUCKET_INCOMING}
    S3BucketBadData:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: ${self:provider.environment.BUCKET_BADDATA}
    S3BucketProcessed:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: ${self:provider.environment.BUCKET_PROCESSED}
    FileDynamoDbTable:
      Type: AWS::DynamoDB::Table
      DeletionPolicy: Retain
      Properties:
        TableName: ${self:provider.environment.TABLE_NAME}
        AttributeDefinitions:
          -
            AttributeName: id
            AttributeType: S
        KeySchema:
          -
            AttributeName: id
            KeyType: HASH
        ProvisionedThroughput:
          ReadCapacityUnits: 1
          WriteCapacityUnits: 1
    ProcessLambdaPermissionIncomingS3:
      Type: AWS::Lambda::Permission
      Properties:
        FunctionName:
          Fn::GetAtt: [ ProcessLambdaFunction, Arn ]
        Principal: "s3.amazonaws.com"
        Action: "lambda:InvokeFunction"
        SourceAccount:
          Ref: AWS::AccountId
        SourceArn:
          Fn::Join: [ ":", [ arn, aws, s3, "", "", "${self:provider.environment.BUCKET_INCOMING}" ]]

  Outputs:
     S3BucketIncoming:
       Description: "name of the S3 bucket for incoming files"
       Value:
         Ref: S3BucketIncoming
     S3BucketBadData:
       Description: "name of the S3 bucket for bad data files"
       Value:
         Ref: S3BucketBadData
     S3BucketProcessed:
       Description: "name of the S3 bucket for processed files"
       Value:
         Ref: S3BucketProcessed
